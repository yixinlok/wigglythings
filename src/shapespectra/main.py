# -*- coding: utf-8 -*-
"""SSS_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdeap8sxdQXVUGOM1YUbO0ZhRnHxMPsR

This notebook is an implementation of our paper
‚ÄúShape Space Spectra‚Äù (https://www.dgp.toronto.edu/projects/sss/), a framework that performs eigenanalysis on shape space.

We‚Äôll start by building a shape space before moving on to the eigenanalysis part.

In the example below, we define a parameterized airplane shape space.
It takes a scalar geometry code as input and outputs a sampled airplane that smoothly changes shape as the code varies.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
from torch.autograd import grad

from .neural_field import MLP_Code
# from shapespectra.airplane import get_airplane
from .tet_mesh import get_shape
import polyscope as ps

def sample(n):
    random_tensor = torch.rand(n, 3) - 0.5
    random_tensor = random_tensor * 2
    return random_tensor

pts = sample(100000)


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#find top k eigen functions
k = 2

models = []
optimizers = []
# Create an instance of the model
for i in range(k):
    model = MLP_Code().to(device)
    total_params = sum(p.numel() for p in model.parameters())

    print("Total parameters in the model:", total_params)
    models.append(model)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    optimizers.append(optimizer)


def getGradient(x, idx):

    outputs = models[idx](x)
    df_ret = None
    for dim in range(outputs.shape[-1]):
        df = grad(outputs=outputs[:, dim], inputs=x, grad_outputs=torch.ones_like(outputs[:, dim]),
                        create_graph=True)[0]
        df = df[:,:3]
        df = df.reshape(-1, 1, 3)
        if(df_ret is None):
           df_ret = df
        else:
           df_ret = torch.cat((df_ret, df), axis = 1)

    return df_ret, outputs


models = []
optimizers = []
for i in range(k):
    model = MLP_Code().to(device)
    total_params = sum(p.numel() for p in model.parameters())

    print("Total parameters in the model:", total_params)
    models.append(model)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    optimizers.append(optimizer)

previous_basis = None
previous_gradient = None

num_epochs = 500
num_samples = 5000

for epoch in range(num_epochs):

    code = torch.rand(1) * (2.4 - 1.8) + 1.8
    geometry_code = code.clone().to(device)
    pts = sample(num_samples)
    # samples = get_airplane(pts, code.item()).to(device)
    samples = get_shape(pts, code.item()).to(device)
    x_train = samples
    code = code.expand(samples.shape[0], 1).to(device)
    x_train = torch.cat((x_train, code), axis = 1)
    x_train.requires_grad_(True)


    df, outputs = getGradient(x_train, 0)
    df = df.detach()
    outputs = outputs.detach()

    U_previous = torch.ones_like(outputs).reshape(-1,1)
    gradient_previous = torch.zeros_like(df).reshape(-1,1)
    loss_total = 0


    energies = None

    # evaluate dirichlet energy to decide the order of optimization
    for idx in range(k):
        df, outputs = getGradient(x_train, idx)
        outputs = outputs.clone().detach()
        df = df.clone().detach()
        energy = torch.sum(df ** 2) / torch.sum(outputs ** 2)
        energy = energy.reshape(-1)

        if energies is None:
              energies = energy
        else:
              energies = torch.cat((energies, energy), axis = -1)

    energies = energies.detach()
    energies, indices = torch.sort(energies) #


    for j in range(k):
        idx = indices[j] # note that the indices are sorted, which means it's order is changing throughout epochs

        df, outputs = getGradient(x_train, idx)
        df = df.reshape(-1,1)

        U_previous = U_previous.detach()
        gradient_previous = gradient_previous.detach()

        UT_previous = U_previous.transpose(1,0).detach()
        lhs = UT_previous @ U_previous
        rhs = UT_previous @ outputs

        q = torch.linalg.lstsq(lhs, rhs).solution
        reconstruct = U_previous @ q                            # can be reperesented by a linear combination from previous basis
        reconstruct_gradient = gradient_previous @ q   # the gradient should also be changed

        outputs = outputs - reconstruct
        df = df - reconstruct_gradient.reshape_as(df)  # also project out the gradient

        loss = torch.sum(df ** 2) / torch.sum(outputs ** 2)
        loss += torch.sum(reconstruct ** 2) / torch.sum(outputs ** 2) * 100
        loss_total += loss.item()

        optimizers[idx].zero_grad()
        loss.backward()
        optimizers[idx].step()

        U_previous = torch.cat((U_previous, outputs), axis = -1)
        gradient_previous = torch.cat((gradient_previous, df), axis = -1)

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}] , k={idx}, eigen: {loss_total:.4f}')


'''
- outputs store the colour (eigenmode)
'''

pts = sample(50000)
for code in np.arange(0.5, 1.5, 0.1):
    # samples = get_airplane(pts, code)
    samples = get_shape(pts, code)
    pts_shape = samples.cpu().detach().numpy()

    code_tensor = torch.tensor([code]).expand(samples.shape[0], 1).to(device)
    x_train = torch.cat((samples, code_tensor), axis = 1).float()

    outputs = models[0](x_train)
    outputs = outputs.detach().cpu().numpy()

    ps.init()
    cloud = ps.register_point_cloud("cloud", pts_shape)
    # colour quantity
    colours = np.zeros((pts_shape.shape[0], 3))
    colours[:, 0] = outputs.reshape(-1)
    colours[:, 2] = 1 - outputs.reshape(-1)
    print("colours shape:", colours.shape)

    cloud.add_color_quantity("eigenmodes", colours, enabled=True)
    ps.show()
    
    # scatter0 = plt.scatter(pts_shape[:, 1].reshape(-1), pts_shape[:, 0].reshape(-1), c=outputs.reshape(-1), cmap='viridis', alpha=0.75, s=2.)
    # # Add color bar to indicate the mapping of color values to function values
    # colorbar0 = plt.colorbar(scatter0)
    # # Set plot title and labels for x and y axes
    # plt.title('Final Solution')
    # plt.xlabel('x')
    # plt.ylabel('y')

    # # Display the plot
    # plt.show()

"""Now we are able to obtain **consistent** eigenmodes across this shape space üôÇ  
The learned basis functions smoothly align across different shapes with no sudden changes.

"""